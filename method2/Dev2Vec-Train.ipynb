{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyWKySRtGF5P",
        "outputId": "46b09510-8e4f-4f88-baec-28ec73a1e5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /home/krg/miniforge3/envs/futrkey1/lib/python3.10/site-packages (2.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pygithub\n",
        "!pip install -q colour-science\n",
        "import colour\n",
        "!pip install emoji\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va0LHAlGPqXG",
        "outputId": "bb4c581e-fd77-4582-f86a-7bee7e0f0c73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/krg/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from functools import reduce\n",
        "import datetime\n",
        "from github import GithubException\n",
        "import pickle\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import gensim\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from github import Github\n",
        "\n",
        "\n",
        "\n",
        "g = Github(login_or_token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8S7TuDIyG0O"
      },
      "source": [
        "#Deps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "reponame = 'vuejs/core'\n",
        "pastDate = (\"2021-01-01\", \"2022-01-01\")\n",
        "currDate = (pastDate[1], \"2023-01-01\")\n",
        "futrDate = (currDate[1], \"2024-01-01\")\n",
        "repo = g.get_repo(reponame)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "reponame = 'spring-projects/spring-security'\n",
        "pastDate = (\"2021-01-01\", \"2022-01-01\")\n",
        "currDate = (pastDate[1], \"2023-01-01\")\n",
        "futrDate = (currDate[1], \"2024-01-01\")\n",
        "repo = g.get_repo(reponame)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reponame = 'go-gitea/gitea'\n",
        "pastDate = (\"2018-01-01\", \"2020-01-01\")\n",
        "currDate = (pastDate[1], \"2022-01-01\")\n",
        "futrDate = (currDate[1], \"2024-01-01\")\n",
        "repo = g.get_repo(reponame)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UZucnwkdWOYb"
      },
      "outputs": [],
      "source": [
        "reponame = 'moby/moby'\n",
        "pastDate = (\"2015-01-01\", \"2018-01-01\")\n",
        "currDate = (pastDate[1], \"2021-01-01\")\n",
        "futrDate = (currDate[1], \"2024-01-01\")\n",
        "repo = g.get_repo(reponame)     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amua6D7UYEKD"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XjJaA5YAswS4"
      },
      "outputs": [],
      "source": [
        "def pload(fname):\n",
        "  arr = []\n",
        "  with open(fname, \"rb\") as f:\n",
        "    arr = pickle.load(f)\n",
        "  return arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#change accordingly first is past 2nd is curr \n",
        "issues = (pload(\"./klones/vuejs_core-2021-01-01..2022-01-01-issues.pkl\"), pload(\"./klones/vuejs_core-2022-01-01..2023-01-01-issues.pkl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DY5D8ka4_oYD"
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(text):\n",
        "    return re.sub(r'[^\\x00-\\x7F]', '', text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\t', ' ')\n",
        "    text = text.replace('\\r', ' ')\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fxSB5O4sE30M"
      },
      "outputs": [],
      "source": [
        "def getTrainDate(arr, train=True):\n",
        "  tagged_data = []\n",
        "  for login, _, title, body, _ in arr:\n",
        "      if title is None or body is None:\n",
        "        continue\n",
        "      text = remove_non_ascii(title) + remove_non_ascii(body)\n",
        "      tokens = preprocess_text(text)\n",
        "      tagged_data.append(TaggedDocument(words=tokens, tags=[login]))\n",
        "  if train:\n",
        "    model = Doc2Vec(tagged_data, vector_size=300, window=5, min_count=1, workers=4)\n",
        "    return tagged_data, model\n",
        "  return tagged_data, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zWpO6UXiGYkx"
      },
      "outputs": [],
      "source": [
        "_, pastModel = getTrainDate(issues[0])\n",
        "_, currModel = getTrainDate(issues[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lXOEuNdzTSe2"
      },
      "outputs": [],
      "source": [
        "pastModel.save(f\"./klones/doc2vec-{reponame.replace('/', '_')}-{pastDate[0]}..{pastDate[1]}-issues\")\n",
        "currModel.save(f\"./klones/doc2vec-{reponame.replace('/', '_')}-{currDate[0]}..{currDate[1]}-issues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#For RQ3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = {key: pastModel.dv[key] for key in pastModel.dv.key_to_index}\n",
        "df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
        "df.to_csv('./author-merge/gitea-6m-doc2vec_embeddings.csv', index_label='key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = {key: currModel.dv[key] for key in currModel.dv.key_to_index}\n",
        "df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
        "df.to_csv('./author-merge/gitea-6m-next-doc2vec_embeddings.csv', index_label='key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "currcsv = pd.read_csv(\"author-merge/gitea-6m-doc2vec_embeddings.csv\")\n",
        "nextcsv = pd.read_csv(\"author-merge/gitea-6m-next-doc2vec_embeddings.csv\")\n",
        "#currdict = currcsv.set_index('key').T.to_dict('list')\n",
        "#nextdict = nextcsv.set_index('key').T.to_dict('list')\n",
        "curr2index = {}\n",
        "for i,name in enumerate(currcsv[\"key\"].to_list()):\n",
        "    curr2index[name] = i\n",
        "index2cemb = []\n",
        "for i in range(len(currcsv)):\n",
        "    index2cemb.append(np.array(currcsv.iloc[i,1:].to_list(), dtype=np.float32))\n",
        "index2cemb = np.array(index2cemb)\n",
        "next2index = {}\n",
        "for i,name in enumerate(nextcsv[\"key\"].to_list()):\n",
        "    next2index[name] = i\n",
        "index2nemb = []\n",
        "for i in range(len(nextcsv)):\n",
        "    index2nemb.append(np.array(nextcsv.iloc[i,1:].to_list(), dtype=np.float32))\n",
        "index2nemb = np.array(index2nemb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.linalg import orthogonal_procrustes\n",
        "\n",
        "common = set(curr2index.keys()) & set(next2index.keys())\n",
        "baseemb = []\n",
        "otheremb = []\n",
        "for c in common:\n",
        "  baseemb.append(index2cemb[curr2index[c]])\n",
        "  otheremb.append(index2nemb[next2index[c]])\n",
        "baseemb = np.array(baseemb).squeeze()\n",
        "otheremb = np.array(otheremb).squeeze()\n",
        "o2b, _ = orthogonal_procrustes(otheremb, baseemb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined = []\n",
        "for d in set(curr2index.keys()) | set(next2index.keys()):\n",
        "    if d not in curr2index:\n",
        "        combined.append((d ,np.concatenate([np.zeros(300), index2nemb[next2index[d]] @ o2b])))\n",
        "    elif d not in next2index:\n",
        "        combined.append((d ,np.concatenate([index2cemb[curr2index[d]], np.zeros(300)])))\n",
        "    else:\n",
        "        combined.append((d ,np.concatenate([index2cemb[curr2index[d]], index2nemb[next2index[d]] @ o2b])))\n",
        "pd.DataFrame(combined).to_csv(\"author-merge/gitea-6m-combined-doc2vec_embeddings.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
